version: "3.8"

services:
  gpt4all_gpu:
    image: ghcr.io/huggingface/text-generation-inference:1.1.1
    container_name: gpt4all_gpu
    restart: always #restart on error (usually code compilation from save during bad state)
    env_file:
      - .env
    environment:
      - HUGGING_FACE_HUB_TOKEN=token
      - USE_FLASH_ATTENTION=true
      - MODEL_ID=${MODEL_ID}
      - NUM_SHARD=${NUM_SHARD}
      - QUANT=${QUANT}
    command: --model-id $MODEL_ID $QUANT --num-shard $NUM_SHARD # Adding syntax to run quantized model
    volumes:
      - './gpt4all_api/models:/data' # Consolidated folder for all LLM models
    ports:
      - "8080:80"
    shm_size: 1g
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]