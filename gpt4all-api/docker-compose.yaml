version: "3.8"

services:
  gpt4all_api:
    image: gpt4all_api
    container_name: gpt4all_api
    restart: always #restart on error (usually code compilation from save during bad state)
    ports:
      - "4891:4891"
    env_file:
      - .env
    environment:
      - APP_ENVIRONMENT=dev
      - WEB_CONCURRENCY=2
      - LOGLEVEL=debug
      - PORT=4891
      - model=${MODEL_BIN} # using variable from .env file
      - inference_mode=${INFERENCE_MODE}
      - LD_LIBRARY_PATH=/usr/local/lib/python3.10/dist-packages/torch/lib
      - IS_DOCKER=1
    volumes:
      - './gpt4all_api/app:/app'
      - './gpt4all_api/models:/models' # models are mounted in the container
    command: ["/start-reload.sh"]
    shm_size: 2g
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
