from typing import List, Iterable, Optional, Dict, Union
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from datetime import datetime
from uuid import uuid4
from api_v1.settings import settings
import aiohttp
import logging
import json
from gpt4all import GPT4All

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

### This should follow https://github.com/openai/openai-openapi/blob/master/openapi.yaml

# Optimzed base model with JSON encoder for datetime
class EncoderBaseModel(BaseModel):
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }

class ChatMessage(EncoderBaseModel):
    role: str
    content: str
# Optional fields can be added here, e.g.:
# timestamp: Optional[datetime] = Field(None, description="Timestamp of the message.")


class ChatCompletionRequest(EncoderBaseModel):
    model: str = Field(settings.model, description="ID of the model to use for chat completion.")
    messages: List[ChatMessage] = Field(..., description="List of chat messages to use as prompt.")

    # Optional fields based on your requirements
    frequency_penalty: Optional[float] = Field(None, description="Penalize new tokens based on frequency in the text so far.")
    #logit_bias: Optional[Dict[str, float]] = Field(None, description="Modify the likelihood of specified tokens appearing in the completion.")
    temperature: Optional[float] = Field(None, description="Temperature for sampling.")
    token_max_length: Optional[int] = Field(None, description="Maximum length of the generated completion (token length).")
    stream: Optional[bool] = Field(None, description="Stream the response as a series of chunks instead of a single response.")
    top_p: Optional[float] = Field(settings.top_p, description="Top-p (nucleus) sampling.")
    top_k: Optional[int] = Field(settings.top_k, description="Top-k sampling.")
        
    # Add other model-specific parameters here
    # Example of an additional optional field
# Debugging ChatCompletionRequest
#logger.debug(f"ChatCompletionRequest: {ChatCompletionRequest}")
    
class ChatCompletionResponseMessage(EncoderBaseModel):
    role: str = Field(..., description="The role of the entity sending the message, typically 'bot' or 'assistant'.")
    content: str = Field(..., description="The text content of the chat response message.")
    finish_reason: Optional[str] = Field(None, description="Explanation of why the model stopped generating the response.")

    # Optional fields can be added here, for example:
    # confidence: Optional[float] = Field(None, description="Confidence score of the model's response.")

class ChatCompletionChoice(EncoderBaseModel):
    message: ChatCompletionResponseMessage = Field(..., description="The chat response message generated by the model.")
    index: int = Field(..., description="The numerical index of this choice among multiple options.")
    finish_reason: str = Field(..., description="The reason why the model finished generating this response.")

class CompletionUsage(EncoderBaseModel):
    prompt_tokens: int = Field(..., description="Number of tokens used in the prompt.")
    completion_tokens: int = Field(..., description="Number of tokens generated as part of the completion.")
    total_tokens: int = Field(..., description="Total number of tokens used.")

    # Optionally, add other fields related to resource usage or model performance

class ChatCompletionResponse(EncoderBaseModel):
    id: str = Field(..., description="Unique identifier for the chat completion response.")
    object: str = Field('text_completion', description="Type identifier for the response object.")
    created: datetime = Field(..., description="Timestamp of when the response was generated.")
    model: str = Field(..., description="The model used for generating the chat completion.")
    choices: List[ChatCompletionChoice] = Field(..., description="List of chat completion choices provided by the model.")
    usage: CompletionUsage = Field(..., description="Details on the token usage for the chat completion.")

# Adding streaming support
class ChatCompletionStreamResponse(EncoderBaseModel):
    id: str = Field(..., description="Unique identifier for the chat completion response.")
    object: str = Field('text_completion', description="Type identifier for the response object.")
    created: datetime = Field(..., description="Timestamp of when this chunk of the response was generated.")
    model: str = Field(..., description="The model used for generating this part of the chat completion.")
    choices: List[ChatCompletionChoice] = Field(..., description="List of chat completion choices provided by the model.")
    index: int = Field(..., description="The numerical index of this chunk in the stream.")
    finish_reason: Optional[str] = Field(None, description="Explanation of why this part of the model's response was generated.")

    # Optionally, add other fields related to this chunk of the streaming response

# Format chat messages
def format_chat_messages(messages: List[ChatMessage]):
    formatted = []
    for msg in messages:
        formatted.append({"role": msg.role, "content": msg.content})
    return json.dumps(formatted[-1])

# Define stream completion response model
def stream_completion(output: Iterable, base_response: ChatCompletionResponse):
    """
    Streams a GPT4All output to the client.

    Args:
        output: The output of GPT4All.generate(), which is an iterable of tokens.
        base_response: The base response object, which is cloned and modified for each token.

    Returns:
        A Generator of CompletionStreamResponse objects, which are serialized to JSON Event Stream format.
    """
    for token in output:
        if isinstance(token, dict):
            content = token.get('content', '')  # Extract the 'content' from the 'token' dictionary
        else:
            content = token  # If 'token' is not a dictionary, use it directly as 'content'
        chunk = base_response.copy()
        chunk.choices = [ChatCompletionChoice(
            message=ChatCompletionResponseMessage(role="system", content=content, finish_reason=''),
            index=0,
            finish_reason=''
        )]
        #print(f"Chunk: {chunk}")  # Debugging print statement
        yield chunk.json()  # Convert the ChatCompletionResponse object to a JSON string




# Define async GPU-compatible completion function
async def gpu_infer(payload, header):
    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(
                settings.gpt4all_api_url,
                headers=header,
                data=payload
            ) as resp:
                return await resp.json()
        except aiohttp.ClientError as e:
            # Handle client-side errors (e.g., connection error, invalid URL)
            logger.error(f"Client error: {e}")
        except aiohttp.ServerError as e:
            # Handle server-side errors (e.g., internal server error)
            logger.error(f"Server error: {e}")
        except json.JSONDecodeError as e:
            # Handle JSON decoding errors
            logger.error(f"JSON decoding error: {e}")
        except Exception as e:
            # Handle other unexpected exceptions
            logger.error(f"Unexpected error: {e}")

#Define router
router = APIRouter(prefix="/chat", tags=["Completions Endpoints"])

@router.post("/completions", response_model=Union[ChatCompletionResponse, None])
async def chat_completions(request: ChatCompletionRequest):
    try:
        # Initialize choices
        choices = []
        formatted_prompt = format_chat_messages(request.messages)
        # Debug formatted_prompt
        #logger.debug(f"Formatted prompt: {formatted_prompt}")
        if settings.inference_mode == "gpu":
            # Prepare payload for GPU inference
            payload = {"parameters": request.dict()}
            header = {"Content-Type": "application/json"}

            response = await gpu_infer(payload, header)

            # Process the response to create ChatCompletionChoice
            for idx, choice in enumerate(response["choices"]):
                choices.append(ChatCompletionChoice(
                    message=ChatCompletionResponseMessage(role="system", content=choice["text"], finish_reason=choice["finish_reason"]),
                    index=idx,
                    finish_reason=choice["finish_reason"]
                ))         
            # Return the response
            return ChatCompletionResponse(
                id=str(uuid4()),
                created=datetime.now(),
                model=request.model,
                choices=choices,
                usage=CompletionUsage(prompt_tokens=0, completion_tokens=0, total_tokens=0)  # Update as needed
            )
        else:
            # CPU inference logic
            model = GPT4All(model_name=request.model, model_path=settings.gpt4all_path)
            #print(f"Model: {model}")  # Debugging print statement
            # Debugging formatted_prompt
            formatted_prompt = format_chat_messages(request.messages)
            #print(f"Formatted prompt: {formatted_prompt}")  # Debugging print statement
            output = model.generate(
                                    prompt=formatted_prompt,
                                    repeat_penalty=request.frequency_penalty,
                                    streaming=request.stream,
                                    temp=request.temperature,
                                    max_tokens=request.token_max_length,
                                    top_p=request.top_p,
                                    top_k=request.top_k,                                    
            )
            # Debugging output
            #print(f"Output: {output}")  # Debugging print statementlogger.debug(f"Output: {output}")
            
            # Stream response logic
            if request.stream:  # Assuming request has a 'stream' attribute to indicate streaming
                base_response = ChatCompletionResponse(
                    id=str(uuid4()),
                    created=datetime.now(),
                    model=request.model,
                    choices=[],
                    usage=CompletionUsage(prompt_tokens=0, completion_tokens=0, total_tokens=0)
                )
                return StreamingResponse(stream_completion(output, base_response), media_type="text/event-stream")
            else:
                # Non-streaming response logic
                choices = [ChatCompletionChoice(
                    message=ChatCompletionResponseMessage(role="system", content=output, finish_reason='model_stop'),
                    index=0,
                    finish_reason='model_stop'
                )]
                return ChatCompletionResponse(
                    id=str(uuid4()),
                    created=datetime.now(),
                    model=request.model,
                    choices=choices,
                    usage=CompletionUsage(
                        prompt_tokens=0,
                        completion_tokens=0,
                        total_tokens=0
                        )
                )
                

    except Exception as e:
        logger.error(f"Error in chat completions: {e}")
        raise HTTPException(status_code=500, detail=str(e))

